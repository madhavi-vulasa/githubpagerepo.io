# -*- coding: utf-8 -*-
"""Appmath2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1jYyFuV8IprL0eo2piSE05tI6DC3of6A8

Fine -Tuning TRANSFORMER Model BERT

TASK SPECIFIC- To find out the level of toxicity in the comments columns in wikipedia which helps in analyzing toxic behaviour in humans.

Source: Dataset: Kaggle - https://www.kaggle.com/competitions/jigsaw-toxic-comment-classification-challenge/data

Installing the transformers library
"""

! pip install transformers -U

!pip install kaggle

from google.colab import files

files.upload()

!cp kaggle.json ~/.kaggle/

"""Importing the dataset for the Analysis"""

!kaggle competitions download -c jigsaw-toxic-comment-classification-challenge

"""#Data preparation"""

import pandas as pd

data = pd.read_csv("/content/drive/MyDrive/jigsaw-toxic-comment-classification-challenge/train.csv", error_bad_lines=False, engine="python")
data.head()

from google.colab import drive
drive.mount('/content/drive')

data

#data.to_csv("/content/drive/MyDrive/jigsaw-toxic-comment-classification-challenge/train.csv", index= False)

data['toxic'].value_counts()

data = data[['comment_text','toxic']]
data.head()

"""Importing the libraries"""

import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score
import torch
from transformers import TrainingArguments, Trainer
from transformers import BertTokenizer, BertForSequenceClassification

"""Tokenization and Model creation"""

from transformers import BertTokenizer, BertForSequenceClassification
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
model = BertForSequenceClassification.from_pretrained('bert-base-uncased',num_labels=2)

model

model = model.to ('cuda')

sample_data = ["I am eating","I am playing "]
tokenizer(sample_data, padding=True, truncation=True, max_length=512)

X = list(data["comment_text"])
y = list(data["toxic"])

"""Applying train-test split"""

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,stratify=y)

X_train_tokenized = tokenizer(X_train, padding=True, truncation=True, max_length=512)

X_test_tokenized = tokenizer(X_test, padding=True, truncation=True, max_length=512)

X_train_tokenized.keys()

print(X_train_tokenized['attention_mask'][0])

len(X_train),len(X_test)

"""Fine -Tuning

Converting the tokenized output in the form of dataset using pytorch.
"""

# Create torch dataset
class Dataset(torch.utils.data.Dataset):
    def __init__(self, encodings, labels=None):
        self.encodings = encodings
        self.labels = labels

    def __getitem__(self, idx):
        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}
        if self.labels:
            item["labels"] = torch.tensor(self.labels[idx])
        return item

    def __len__(self):
        return len(self.encodings["input_ids"])

train_dataset = Dataset(X_train_tokenized, y_train)
val_dataset = Dataset(X_test_tokenized, y_test)

train_dataset[5]

def compute_metrics(p):
    print(type(p))
    pred, labels = p
    pred = np.argmax(pred, axis=1)

    accuracy = accuracy_score(y_true=labels, y_pred=pred)
    recall = recall_score(y_true=labels, y_pred=pred)
    precision = precision_score(y_true=labels, y_pred=pred)
    f1 = f1_score(y_true=labels, y_pred=pred)

    return {"accuracy": accuracy, "precision": precision, "recall": recall, "f1": f1}

"""Defining a trainer"""

# Define Trainer for 1 epoch
args = TrainingArguments(
    output_dir="output",
    num_train_epochs=1,
    per_device_train_batch_size=8

)
trainer = Trainer(
    model=model,
    args=args,
    train_dataset=train_dataset,
    eval_dataset=val_dataset,
    compute_metrics=compute_metrics
)

trainer.train()

trainer.evaluate()

#np.set_printoptions(suppress=True)

"""Predictions"""

#text = "That was good point"
text= "go to hell"
inputs = tokenizer(text,padding = True, truncation = True, return_tensors='pt').to('cuda')
outputs = model(**inputs)
print(outputs)
predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)
print(predictions)
predictions = predictions.cpu().detach().numpy()
predictions
# here near to 1 -toxic comment whereas 0 is good comment



"""Saving the model"""

trainer.save_model('CustomModel')

# trainer.save_model('/content/drive/MyDrive/Youtube Tutorials/toxic')
# model_2 = BertForSequenceClassification.from_pretrained("/content/drive/MyDrive/Youtube Tutorials/toxic")
# model_2.to('cuda')

model_2= BertForSequenceClassification.from_pretrained('bert-base-uncased',num_labels=2)

# Define Trainer for 5 epochs- large-original dataset
args2= TrainingArguments(
    output_dir="output",
    num_train_epochs=5,
    per_device_train_batch_size=8

)
trainer2 = Trainer(
    model=model_2,
    args=args2,
    train_dataset=train_dataset,
    eval_dataset=val_dataset,
    compute_metrics=compute_metrics
)

trainer2.train()

trainer2.evaluate()

model_2 = BertForSequenceClassification.from_pretrained("CustomModel")
model_2.to('cuda')

# text = "That was good point"
text = "go to hell"
inputs = tokenizer(text,padding = True, truncation = True, return_tensors='pt').to('cuda')
outputs = model_2(**inputs)
predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)
predictions = predictions.cpu().detach().numpy()
predictions

"""After running epochs for 5 , the disk space was filled up even though the google collab units were purchased,hence we minimized out length of the dataset to a sample size of 1000 considering 80% train and 20% test, to check for the accuracy of the comment to be labelled as toxic or not."""

#Considering sample size of 1000
data = data[['comment_text','toxic']]
mod_data=data[0:1000]
mod_data.head()

tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

model3 = BertForSequenceClassification.from_pretrained('bert-base-uncased',num_labels=2)

model3 = model.to ('cuda')

sample_data2 = ["I am eating","I am playing "]
tokenizer(sample_data, padding=True, truncation=True, max_length=512)

X = list(mod_data["comment_text"])
y = list(mod_data["toxic"])

X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2,stratify=y)
X_train_tokenized = tokenizer(X_train, padding=True, truncation=True, max_length=512)
X_val_tokenized = tokenizer(X_val, padding=True, truncation=True, max_length=512)

X_train_tokenized.keys()

print(X_train_tokenized['attention_mask'][0])

len(X_train),len(X_val)

# Create torch dataset
class Dataset(torch.utils.data.Dataset):
    def __init__(self, encodings, labels=None):
        self.encodings = encodings
        self.labels = labels

    def __getitem__(self, idx):
        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}
        if self.labels:
            item["labels"] = torch.tensor(self.labels[idx])
        return item

    def __len__(self):
        return len(self.encodings["input_ids"])

train_dataset2 = Dataset(X_train_tokenized, y_train)
val_dataset2 = Dataset(X_val_tokenized, y_val)

def compute_metrics(p):
    print(type(p))
    pred, labels = p
    pred = np.argmax(pred, axis=1)

    accuracy = accuracy_score(y_true=labels, y_pred=pred)
    recall = recall_score(y_true=labels, y_pred=pred)
    precision = precision_score(y_true=labels, y_pred=pred)
    f1 = f1_score(y_true=labels, y_pred=pred)

    return {"accuracy": accuracy, "precision": precision, "recall": recall, "f1": f1}

# Define Trainer for mod_data for 10 epoch
args3 = TrainingArguments(
    output_dir="output",
    num_train_epochs=10,
    per_device_train_batch_size=8

)
trainer3 = Trainer(
    model=model3,
    args=args3,
    train_dataset=train_dataset2,
    eval_dataset=val_dataset2,
    compute_metrics=compute_metrics
)

trainer3.train()

trainer3.evaluate()

text1= "That was good point"
text2 = "go to hell"
inputs = tokenizer(text,padding = True, truncation = True, return_tensors='pt').to('cuda')
outputs = model3(**inputs)
predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)
predictions = predictions.cpu().detach().numpy()
predictions
# Here near to 1 is the indication of a toxic comment whereas near to 0 is a good comment.

# Define Trainer for mod_data for 10 epoch
args4 = TrainingArguments(
    output_dir="output",
    num_train_epochs=50,
    per_device_train_batch_size=8

)
trainer4 = Trainer(
    model=model3,
    args=args4,
    train_dataset=train_dataset2,
    eval_dataset=val_dataset2,
    compute_metrics=compute_metrics
)

trainer4.train()

"""Results:

The scores considering the large dataset and the modified one are as follows:

For Epoch 1 -large dataset  <class 'transformers.trainer_utils.EvalPrediction'>
{'eval_loss': 0.5039833188056946,
 'eval_accuracy': 0.9001409995300016,
 'eval_precision': 0.11904761904761904,
 'eval_recall': 0.006538084341288003,
 'eval_f1': 0.012395413696932136,
 'eval_runtime': 1064.7262,
 'eval_samples_per_second': 29.975,
 'eval_steps_per_second': 3.747}

 For Epoch 10- modified dataset
 <class 'transformers.trainer_utils.EvalPrediction'>
{'eval_loss': 0.8390256762504578,
 'eval_accuracy': 0.105,
 'eval_precision': 0.105,
 'eval_recall': 1.0,
 'eval_f1': 0.19004524886877827,
 'eval_runtime': 9.0426,
 'eval_samples_per_second': 22.117,
 'eval_steps_per_second': 2.765}


 Comparitive analysis shows that when trained on a larger data and smaller dataset the scores were accurate when the epoches were higher and helpful in identifying the toxic comments.

near to 1 is toxic comment and 0 is good comment.
Epoch 1 -larger datset
SequenceClassifierOutput(loss=None, logits=tensor([[-1.2539,  1.4588]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)
tensor([[0.0622, 0.9378]], device='cuda:0', grad_fn=<SoftmaxBackward0>)
array([[0.06223044, 0.9377696 ]], dtype=float32)


Epoch 10 - modified dataset -array([[0.01006557, 0.9899344 ]], dtype=float32)
"""